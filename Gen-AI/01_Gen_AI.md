# Generative AI :
- Generative AI (Gen AI) refers to a branch of **artificial intelligence** that focuses on creating new contentâ€”such as text, images, audio, video, or codeâ€”rather than just analyzing or classifying existing data.

- **It is a method in AI that teaches computers to process the data in way that is inspired by a human brain.**

## ðŸ”¹ 1. Core Concepts :

- **Generative Models** : Models that can create new data similar to the data they were trained on.

- **Deep Learning** : Especially neural networks like transformers, GANs, VAEs.

- **Unsupervised / Self-supervised Learning** : Learning from unlabeled data is crucial in Gen AI.

## 2. Types of Generative AI Models :

- **Transformers** : Used in models like GPT (text), DALLÂ·E (images), Codex (code).

- **GANs (Generative Adversarial Networks)** : Used in realistic image, video, and audio generation.

- **VAEs (Variational Autoencoders)** : Often used in image generation.

- **Diffusion Models** : Used in high-quality image and video synthesis (e.g., Stable Diffusion, Imagen).

## 3. Modalities (What Gen AI Can Create) :

- **Text** : Chatbots, stories, summaries (e.g., ChatGPT).

- **Images** : Art, illustrations, image editing (e.g., DALLÂ·E, Midjourney).

- **Audio** : Music, voice synthesis (e.g., ElevenLabs, Suno).

- **Video** : Short clips, animations (e.g., Runway, Sora).

- **Code** : Code generation, code completion (e.g., GitHub Copilot).

- **3D Objects** : Used in gaming, virtual reality.

# GPT : Generative Pre-Trained Transformer
- GPT = a language model built on top of this transformer architecture, **trained to generate human-like text**.

## Generative :
- Whatever our **input we give to llm model** will generate something that's called <mark>**Generative**</mark>.

## Pre-Trained :
- All the things are pre-trained in gpt model.

## Transformer :
- is a powerful architecture that uses self-attention to understand context.